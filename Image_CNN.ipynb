{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf05af9",
   "metadata": {},
   "source": [
    "# CNN (Convolutional Neural Network)\n",
    "\n",
    "- 참고\n",
    "    - Image classification with deep convolutional neural networks (Khan et al., 2020)\n",
    "    - ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d4bd9",
   "metadata": {},
   "source": [
    "- Convolutional Neural Network(CNN)은 컴퓨터 비전 및 이미지 처리와 관련된 여러 대회에서 우수한 성능을 보인 특별한 유형의 신경망\n",
    "- CNN은 이미지 내의 특징을 추출하고 이를 이용해 이미지를 분류하거나 객체를 검출하는 등의 작업을 수행할 수 있음\n",
    "- CNN은 입력 이미지를 여러 개의 레이어로 처리하며, 각 레이어에서는 입력 이미지와 필터를 합성곱하여 특징 맵(feature map)을 생성\n",
    "- 이때 필터는 이미지에서 특정한 패턴을 찾아내기 위한 가중치 값으로, 학습 과정에서 자동으로 조정됨\n",
    "- 레이어를 여러 번 거침으로써, 점점 더 추상적인 특징을 추출할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cb7e0",
   "metadata": {},
   "source": [
    "- ImageNetClassification with Deep Convolutional Neural Networks에서는 8개의 레이어로 이루어진 CNN 아키텍처인 AlexNet을 제안함\n",
    "- 이 아키텍처는 풀링 레이어, 컨볼루션 레이어, 활성화 함수, 드롭아웃 레이어 등 다양한 컴포넌트로 구성됨\n",
    "- 또한, 큰 데이터셋과 병렬 컴퓨팅을 이용하여 CNN의 학습을 가속화하는 방법도 제안하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deeca25",
   "metadata": {},
   "source": [
    "- AlexNet은 기존의 신경망 아키텍처에 비해 몇 가지 차이점이 있음\n",
    "    1. 더 많은 학습 데이터 사용: AlexNet은 120만 개 이상의 이미지를 사용하여 학습\n",
    "    2. GPU를 사용한 병렬 처리: AlexNet은 2개의 GPU를 사용하여 효율적으로 학습할 수 있도록 설계\n",
    "    3. ReLU 활성화 함수 사용: 기존의 신경망에서는 sigmoid 함수나 tanh 함수를 사용했지만, AlexNet에서는 ReLU(Rectified Linear Unit) 함수를 사용하여 학습 속도를 높임\n",
    "    4. Local Response Normalization: 이전 레이어에서 활성화된 뉴런들이 다음 레이어에서 높은 활성화 값을 가지도록 정규화를 적용\n",
    "    5. Dropout: 학습 데이터에서 무작위로 뉴런을 제거하면서 학습하는 방법으로, 오버피팅(overfitting)을 방지하는 데 효과적\n",
    "- 논문에서는 또한 maxpooling과 conv2d도 자세히 설명하고 있음\n",
    "    - Maxpooling은 이미지의 크기를 줄이고, 불필요한 정보를 제거하는 데 사용됨\n",
    "    - Conv2d는 이미지에서 특징을 추출하는 데 사용되는 컨볼루션 필터를 적용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41531ec",
   "metadata": {},
   "source": [
    "<img src = \"./image/CNN.png\">\n",
    "\n",
    "- 합성곱 : 입력 데이터에서 유용한 특성만 드러나게 하는 것\n",
    "    - 합성곱 계산을 통해 만들어진 출력을 특성맵(feature map)이라고 부름\n",
    "    \n",
    "- 일반적으로 1개 이상의 합성곱 층을 사용한 인공 신경망을 합성곱 신경망이라고 부름\n",
    "    - 합성곱 층만을 이용한 신경망은 아님\n",
    "    \n",
    "- 합성곱과 밀집층의 차이\n",
    "    - 밀집층에서는 뉴런마다 입력 개수만큼의 가중치가 존재\n",
    "    - 합성곱은 입력 데이터의 일부에 가중치를 곱함\n",
    "    - 합성곱 신경망에서는 유닛을 필터(filter) 또는 커널(kernel)이라고 부름\n",
    "    \n",
    "- 입력데이터가 2차원 배열이라면 커널도 2차원이어야함\n",
    "- 위 이미지에서 커널 크기는 (3, 3)\n",
    "- 밀집층에서 여러 개의 뉴런을 사용하듯 합성곱 층에서도 여려 개의 필터를 사용할 수 있음\n",
    "- 합성곱은 2차원의 형태를 유지함\n",
    "    -  공간적 특성의 손실을 줄일 수 있기 때문에 이미지 처리 분야에서 성능이 뛰어남\n",
    "    \n",
    "    \n",
    "- 합성곱 연산 순서\n",
    "    1. 왼쪽 위 모서리부터 합성곱을 시작\n",
    "    2. 1개의 출력을 계산\n",
    "    3. 오른쪽으로 이동\n",
    "        - 오른쪽으로 이동 할 수 없으면 아래로 이동\n",
    "    4. 2 ~ 3의 과정을 반복해 합성곱 연산을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bcb22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25547168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.conv2d.Conv2D at 0x1ed685f5f90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 합성곱\n",
    "keras.layers.Conv2D(10,\n",
    "                    kernel_size = (3, 3),\n",
    "                    activation = \"relu\",\n",
    "                    padding = \"same\",\n",
    "                    strides = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb2ccb",
   "metadata": {},
   "source": [
    "- Conv2D의 첫 번째 매개변수는 필터의 개수(필수)\n",
    "- kernel_size = 필터의 크기(필수)\n",
    "    - 일반적으로 (3, 3)이나 (5, 5)의 크기로 사용\n",
    "- 활성화 함수\n",
    "    - 합성곱 신경망에서 특성맵은 절편과 활성화 함수를 적용한 후의 결과물"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045dcc82",
   "metadata": {},
   "source": [
    "## 패딩\n",
    "\n",
    "<img src = \"./image/padding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30531b59",
   "metadata": {},
   "source": [
    "- 입력 배열의 주위를 가상의 원소로 채우는 것\n",
    "- 합성곱층을 통과하면 출력의 크기가 입력데이터의 크기보다 작아지게 되는데 이것을 방지하기 위해 사용\n",
    "    - 마치 (4, 4)보다 더 큰 입력데이터가 들어온 것 처럼 계산\n",
    "    - (6, 6)의 데이터를 (3, 3)크기의 커널로 합성곱 연산을 하면 출력의 크기가 (4, 4)로 유지됨\n",
    "    \n",
    "- 실제로는 입력값이 아니기 때문에 패딩은 0으로 채움\n",
    "    - 값이 0으로 채워져 있기 때문에 계산에 영향을 미치지 않음\n",
    "    \n",
    "    \n",
    "- 세임 패딩(same padding) : 입력과 특성맵의 크기를 동일하게 만들기 위해 입력 데이터 주위에 0으로 패딩하는 것\n",
    "    - 일반적으로는 세임 패딩이 많이 사용됨\n",
    "    \n",
    "- 밸리드 패딩(valid padding) : 패딩 없이 순수한 입력배열에서만 합성곱을 해 특성맵을 만드는 것\n",
    "    - 특성 맵의 크기가 입력보다 줄어듦\n",
    "    \n",
    "\n",
    "- 패딩을 사용하는 이유\n",
    "    - 패딩을 사용하지 않으면 입력값의 가운데에 있는 원소와 모서리 부분의 사용 비율이 크게 차이남\n",
    "    - 적절하게 패딩을 사용하면 이미지 주변의 정보 소실을 막을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b31c2d",
   "metadata": {},
   "source": [
    "## 스트라이드\n",
    "\n",
    "<img src = \"./image/stride.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ba051",
   "metadata": {},
   "source": [
    "- 필터를 적용하는 위치의 간격(이동의 크기)\n",
    "- 기본값은 1\n",
    "    - 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 (1, 1)과 같이 튜플로 각각 지정할 수 있음\n",
    "    - 일반적으로 가로세로의 크기를 똑같이 지정\n",
    "    - 1보다 큰 스트라이드를 사용하는 경우도 드문 편이었으나\n",
    "        - 최근에는 maxpooling을 사용하지 않고 스트라이드를 높이는 방식이 점점 도입됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2e28f",
   "metadata": {},
   "source": [
    "## 풀링\n",
    "\n",
    "<img src = \"./image/pooling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761462c",
   "metadata": {},
   "source": [
    "- 합성곱 층에서 만든 특성앱의 가로세로 크기를 줄이는 역할을 수행\n",
    "    - 특성맵의 크기는 변하지 않음\n",
    "    \n",
    "- 합성곱 층에서 추출한 특징을 유지하면서 계산량을 줄여주고 다음 계층 신경망과 쉽게 연결해주기 위해서 사용\n",
    "- 풀링에는 가중치가 없고 최댓값을 계산하거나(max pooling) 평균값을 계산(average pooling)\n",
    "    - average pooling보다는 max pooling이 많이 사용됨\n",
    "    - average pooling은 특성맵의 중요한 정보를 평균 계산하는 과정에서 희석될 수 있기 때문에\n",
    "    \n",
    "- 합성곱은 커널이 겹치는 부분이 있지만 풀링은 겹치지 않고 이동\n",
    "\n",
    "- 최근에는 신경망이 점점 깊어지면서 미세하지만 중요한 특징들이 소실되는 현상을 막기 위해서 풀링층 사용을 줄이는 방식도 도입되고 있음\n",
    "    - 하지만 여전히 풀링은 적은 계산량으로 좋은 성능을 유지하는 데에 유용한 신경망 계층임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcae07c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x1ed6b020040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 풀링\n",
    "keras.layers.MaxPool2D(2, strides = 2, padding = \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506808e",
   "metadata": {},
   "source": [
    "- MaxPool2D의 첫 번째 매개변수는 풀링의 크기\n",
    "    - 일반적으로 2를 사용(가로세로 크기를 절반으로 줄임)\n",
    "    - 가로세로 방향의 풀링 크기를 다르게 하려면 튜플로 입력가능하지만 실제로 사용되는 경우는 매우 드문 편\n",
    "    \n",
    "- strides : 자동으로 풀링의 크기가 입력되기 때문에 따로 지정할 필요는 없음\n",
    "- padding : 기본값은 valid. 보통은 풀링에서 패딩을 하지 않기 때문에 따로 지정하는 경우는 거의 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5336651d",
   "metadata": {},
   "source": [
    "# 합성곱 신경망의 구조\n",
    "\n",
    "<img src = \"./image/cnn구조.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9785b1",
   "metadata": {},
   "source": [
    "- 합성곱 신경망은 너비와 높이는 점점 줄어들고 깊이는 점점 깊어지는 것이 특징\n",
    "- 마지막 분류 신경망에서 특성맵을 모두 펼쳐서 밀집층의 입력으로 사용\n",
    "\n",
    "- 합성곱 신경망에서 필터는 이미지에 있는 어떤 특징을 찾는 역할\n",
    "    - 필터의 개수를 늘릴수록, 층이 깊어질수록 데이터의 구체적인 특징을 감지\n",
    "    - 어떤 특징이 이미지의 어느 위치에 놓이더라도 쉽게 감지할 수 있도록 너비와 높이를 압축(풀링)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17d62d",
   "metadata": {},
   "source": [
    "# 컬러 이미지를 사용한 합성곱\n",
    "\n",
    "<img src = \"./image/conv_channel.jpg\" width = 500 height = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a907b",
   "metadata": {},
   "source": [
    "- 흑백 이미지는 2차원 배열로 표현할 수 있음\n",
    "- 컬러 이미지는 RGB(빨강, 초록, 파랑) 채널로 구성되어 있기 때문에 하나의 이미지를 3차원 배열로 표시\n",
    "\n",
    "- 깊이가 있는 입력에서 합성곱을 수행하기 위해서는 커널도 깊이가 있어야 함(3차원으로 구성되어야 함)\n",
    "    - 커널 배열의 깊이는 항상 입력의 깊이와 같음\n",
    "    \n",
    "- 입력이나 필터의 차원이 몇 개인지와 관계없이 항상 출력은 하나의 값\n",
    "- 케라스의 합성곱층은 기본적으로 3차원 입력에 맞춰져있음\n",
    "    - 흑백 이미지는 깊이 차원이 1인 3차원 배열로 변환하여 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1762e24",
   "metadata": {},
   "source": [
    "# 실습\n",
    "- 캐글(Kaggle)의 개 고양이 이미지 분류 대회의 이미지 데이터셋을 이용한 CNN 이미지 분류\n",
    "    - https://www.kaggle.com/c/dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfb0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 실행에 필요한 메시지 외 경고가 출력되지 않게 해줌\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13faa7",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDataGenerator = ImageDataGenerator(\n",
    "    rescale=1./255,                  # 고정 이미지 - 픽셀값을 0~255에서 0~1범위로 변경\n",
    "    shear_range=0.2,                 # 기울기 범위\n",
    "    zoom_range=0.2,                  # 확대 범위\n",
    "    horizontal_flip=True,           # 상하반전\n",
    "    validation_split=0.1            # 배치비율(예: 0.3, 0.2, 0.25 등등)\n",
    ")\n",
    "imageDataGenerator_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dataset = imageDataGenerator.flow_from_directory(\n",
    "    \"./data/train/\",\n",
    "    target_size=(128,128),\n",
    "    batch_size=100,\n",
    "    subset=\"training\",\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_dataset = imageDataGenerator.flow_from_directory(\n",
    "    \"./data/train/\",\n",
    "    target_size=(128,128),\n",
    "    batch_size=100,\n",
    "    subset=\"validation\",\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_dataset = imageDataGenerator_test.flow_from_directory(\n",
    "    \"./data/test/\",\n",
    "    target_size=(128,128),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28348e07",
   "metadata": {},
   "source": [
    "- 개 이미지 12,500개, 고양이 이미지 12,500개를 train/validation/test 용으로 분리하고, tensorflow의 ImageDataGenerator를 사용할 것이기 때문에 각 라벨(cat,dog)를 폴더별로 따로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91445c3f",
   "metadata": {},
   "source": [
    "## 모델 생성 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de75ca28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T07:13:44.205487Z",
     "start_time": "2023-02-07T07:13:44.062194Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 128, 128, 8)       224       \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 64, 64, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 64, 64, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 32, 32, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                131104    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 155,665\n",
      "Trainable params: 155,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(8, kernel_size = 3, activation = \"relu\", padding = \"same\",\n",
    "                              input_shape = (128,128, 3)))\n",
    "model.add(keras.layers.MaxPool2D(2))\n",
    "model.add(keras.layers.Conv2D(16, kernel_size = 3, activation = \"relu\", padding = \"same\"))\n",
    "model.add(keras.layers.MaxPool2D(2))\n",
    "model.add(keras.layers.Conv2D(32, kernel_size = 3, activation = \"relu\", padding = \"same\"))\n",
    "model.add(keras.layers.MaxPool2D(2))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size = 3, activation = \"relu\", padding = \"same\"))\n",
    "model.add(keras.layers.MaxPool2D(2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(0.4))\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8be09020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T07:13:49.734209Z",
     "start_time": "2023-02-07T07:13:49.730703Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"./model/best-cnn-3model.h5\", save_best_only = True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9958020f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T07:13:50.455091Z",
     "start_time": "2023-02-07T07:13:50.445106Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6a2e0fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T08:22:05.715369Z",
     "start_time": "2023-02-07T07:13:51.316667Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "225/225 [==============================] - 201s 885ms/step - loss: 0.6478 - accuracy: 0.6076 - val_loss: 0.5838 - val_accuracy: 0.6764\n",
      "Epoch 2/100\n",
      "225/225 [==============================] - 186s 826ms/step - loss: 0.5646 - accuracy: 0.7076 - val_loss: 0.5241 - val_accuracy: 0.7272\n",
      "Epoch 3/100\n",
      "225/225 [==============================] - 185s 822ms/step - loss: 0.5202 - accuracy: 0.7446 - val_loss: 0.4835 - val_accuracy: 0.7792\n",
      "Epoch 4/100\n",
      "225/225 [==============================] - 185s 824ms/step - loss: 0.4873 - accuracy: 0.7692 - val_loss: 0.4540 - val_accuracy: 0.7804\n",
      "Epoch 5/100\n",
      "225/225 [==============================] - 183s 814ms/step - loss: 0.4716 - accuracy: 0.7815 - val_loss: 0.4525 - val_accuracy: 0.7968\n",
      "Epoch 6/100\n",
      "225/225 [==============================] - 186s 826ms/step - loss: 0.4450 - accuracy: 0.7982 - val_loss: 0.4028 - val_accuracy: 0.8128\n",
      "Epoch 7/100\n",
      "225/225 [==============================] - 189s 840ms/step - loss: 0.4263 - accuracy: 0.8103 - val_loss: 0.3936 - val_accuracy: 0.8232\n",
      "Epoch 8/100\n",
      "225/225 [==============================] - 184s 819ms/step - loss: 0.4113 - accuracy: 0.8191 - val_loss: 0.3656 - val_accuracy: 0.8368\n",
      "Epoch 9/100\n",
      "225/225 [==============================] - 187s 829ms/step - loss: 0.4000 - accuracy: 0.8270 - val_loss: 0.3604 - val_accuracy: 0.8396\n",
      "Epoch 10/100\n",
      "225/225 [==============================] - 185s 823ms/step - loss: 0.3812 - accuracy: 0.8368 - val_loss: 0.3512 - val_accuracy: 0.8412\n",
      "Epoch 11/100\n",
      "225/225 [==============================] - 186s 826ms/step - loss: 0.3700 - accuracy: 0.8411 - val_loss: 0.3625 - val_accuracy: 0.8340\n",
      "Epoch 12/100\n",
      "225/225 [==============================] - 186s 828ms/step - loss: 0.3519 - accuracy: 0.8508 - val_loss: 0.3225 - val_accuracy: 0.8520\n",
      "Epoch 13/100\n",
      "225/225 [==============================] - 187s 829ms/step - loss: 0.3441 - accuracy: 0.8566 - val_loss: 0.3323 - val_accuracy: 0.8588\n",
      "Epoch 14/100\n",
      "225/225 [==============================] - 186s 825ms/step - loss: 0.3334 - accuracy: 0.8581 - val_loss: 0.3042 - val_accuracy: 0.8652\n",
      "Epoch 15/100\n",
      "225/225 [==============================] - 184s 817ms/step - loss: 0.3233 - accuracy: 0.8662 - val_loss: 0.2984 - val_accuracy: 0.8716\n",
      "Epoch 16/100\n",
      "225/225 [==============================] - 183s 814ms/step - loss: 0.3112 - accuracy: 0.8694 - val_loss: 0.2926 - val_accuracy: 0.8756\n",
      "Epoch 17/100\n",
      "225/225 [==============================] - 184s 817ms/step - loss: 0.2991 - accuracy: 0.8784 - val_loss: 0.2995 - val_accuracy: 0.8764\n",
      "Epoch 18/100\n",
      "225/225 [==============================] - 184s 819ms/step - loss: 0.2991 - accuracy: 0.8780 - val_loss: 0.2730 - val_accuracy: 0.8840\n",
      "Epoch 19/100\n",
      "225/225 [==============================] - 184s 819ms/step - loss: 0.2828 - accuracy: 0.8840 - val_loss: 0.2719 - val_accuracy: 0.8828\n",
      "Epoch 20/100\n",
      "225/225 [==============================] - 184s 818ms/step - loss: 0.2885 - accuracy: 0.8816 - val_loss: 0.2973 - val_accuracy: 0.8708\n",
      "Epoch 21/100\n",
      "225/225 [==============================] - 184s 818ms/step - loss: 0.2663 - accuracy: 0.8924 - val_loss: 0.2838 - val_accuracy: 0.8828\n",
      "Epoch 22/100\n",
      "225/225 [==============================] - 189s 840ms/step - loss: 0.2655 - accuracy: 0.8919 - val_loss: 0.2763 - val_accuracy: 0.8812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs = 100, validation_data = val_dataset,\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb])\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f8a1ea5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T08:22:42.585136Z",
     "start_time": "2023-02-07T08:22:05.718405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 37s 291ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
