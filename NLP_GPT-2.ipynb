{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb8b2ef",
   "metadata": {},
   "source": [
    "# GPT-2 (Generative Pre-Training of a language model)\n",
    "- 참고\n",
    "    - GPT-2: Language Models are Unsupervised Multitask Learners (Radford et. al., 2019)\n",
    "    - GPT-2 논문 리뷰 (https://youtu.be/8hd2Q-3-BsQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa60756",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Current System\n",
    "- Machine Learning 시스템은 현재 large datasets, high-capacity models, 지도 학습의 조합을 사용하여 학습한 작업에서 우수한 성능을 발휘\n",
    "- 그러나 이러한 시스템은 데이터 분포나 task specifications의 작은 변화에 민감하며 취약\n",
    "- 즉, 현재의 시스템은 전문가보다는 narrow experts에 가까움 \n",
    "- 논문에서는 각 작업을 수동으로 생성하고 레이블링 할 필요 없이 많은 작업을 수행할 수 있는 더 일반적인 시스템으로 구축하고자 함\n",
    "\n",
    "\n",
    "- ML 시스템을 만드는 주요 방식은, 원하는 작업의 올바른 동작을 보여주는 학습 예제 데이터셋을 수집하고, \n",
    "- 이러한 동작을 모방하도록 시스템을 학습시킨 다음 독립적이고 동일하게 분포된(IID) 예제들에서 성능을 테스트하는 것\n",
    "- 이 방법은 전문가를 만들어 발전하는 데 큰 역할을 해왔으나 단점이 존재함\n",
    "\n",
    "\n",
    "- 논문에서는 단일 도메인 데이터셋에서 단일 작업 학습이 보편화되어 있는 것이 현재 시스템의 일반화 부족의 주요 원인 중 하나라고 제시\n",
    "- 현재 아키텍처에서 강건한 시스템으로의 발전을 위해서는 다양한 도메인에서 학습하고 성능을 측정하는 것이 필요\n",
    "    - GLUE(Wang et al., 2018) 및 decaNLP(McCann et al., 2018)와 같은 몇 가지 벤치마크가 제안되어 이를 연구하기 시작\n",
    "    \n",
    "\n",
    "### 1.1.1. Multitask learning\n",
    "- Multitask learning은 성능을 향상시키는 데 유리한 프레임워크. 그러나 NLP에서의 다중 작업 학습은 아직 초기 단계\n",
    "- 최근 연구에서는 성능 향상이 미약하게 보고되고 있으며, 현재까지 가장 유의한 노력은 각각 10개와 17개의 (데이터셋, 목적) 쌍으로 훈련한 것(McCann et al., 2018)(Bowman et al., 2018)\n",
    "- 메타러닝 관점에서 각 (데이터셋, 목적) 쌍은 데이터셋 및 목적 분포에서 샘플링 된 단일 학습 예제\n",
    "- 현재 머신러닝 시스템은 잘 일반화되는 함수를 유도하기 위해 수백 개에서 수천 개의 예제가 필요\n",
    "- 이는 현재 방법으로 목적을 실현하기 위해서는 다수의 효과적인 학습 쌍이 필요할 수 있다는 것을 시사\n",
    "- 현재 기술로 데이터셋을 생성하고 목적을 설계하는 규모를 지속적으로 확장하는 것은 매우 어려움\n",
    "- 이는 다중 작업 학습을 수행하기 위한 추가적인 설정을 탐색하는 동기를 제공함\n",
    "\n",
    "\n",
    "### 1.1.2. pre-training and fine-tuning\n",
    "- 현재 언어 작업에서 최고의 성능을 내는 시스템들은 사전 훈련과 지도 학습 fine-tuning의 조합을 활용\n",
    "- 이러한 방법은 더 유연한 transfer 학습 방식으로 향하는 추세를 가지고 있음\n",
    "- 먼저, 단어 벡터가 학습되고 작업 특화 아키텍처의 입력으로 사용 (Mikolov et al., 2013) (Collobert et al., 2011)\n",
    "- 그 후, 순환 신경망의 문맥적 표현이 transfer됨 (Dai & Le, 2015) (Peters et al., 2018)\n",
    "- 최근 연구는 작업 특화 아키텍처가 더 이상 필요하지 않으며 많은 self-attention 블록을 transfer하는 것만으로 충분하다는 것을 제안 (Radford et al., 2018) (Devlin et al., 2018)\n",
    "\n",
    "\n",
    "### 1.1.3. So...\n",
    "- 이러한 방법들은 여전히 작업을 수행하기 위해 감독된 학습이 필요\n",
    "- 만약 최소한의 지도 학습 데이터가 없거나 전혀 없는 경우, 언어 모델을 사용하여 공감 추론과 같은 특정 작업을 수행할 수 있다는 것을 보여주는 다른 분야의 연구가 있음\n",
    "    - Schwartz et al. (2017) 및 Radford et al. (2017)는 감성 분석과 같은 작업을 수행하는 데 언어 모델의 가능성을 보여주는 연구를 실시\n",
    "\n",
    "- 본 논문에서는 이러한 두 가지 연구 분야를 연결하며 transfer 학습 방법론의 일반화 경향을 지속\n",
    "- 언어 모델이 어떠한 매개 변수 또는 구조 수정 없이 zero-shot 설정에서 하위 작업을 수행할 수 있는 능력을 보여줌\n",
    "- 이 접근 방식은 zero-shot 설정에서 다양한 작업을 수행하는 언어 모델의 잠재력을 강조함으로써 가능성을 보여주고 있음\n",
    "\n",
    "<img src = \"./image/gpt2_zeroshot.png\">\n",
    "- 다양한 NLP 작업에 대한 모델 크기에 따른 WebText 언어 모델의 Zero-shot task 성능\n",
    "- Reading Comprehension은 CoQA (Reddy et al., 2018)에서,  translation은 WMT-14 Fr-En (Artetxe et al., 2017)에서, summarization은 CNN 및 Daily Mail (See et al., 2017)에서, 그리고 Question Answering은 Natural Questions (Kwiatkowski et al., 2019)에서 수행됨\n",
    "- 각 결과에 대한 자세한 설명은 섹션 3에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94787e",
   "metadata": {},
   "source": [
    "# 2. Approach\n",
    "\n",
    "- 본 논문에서 접근법의 핵심은 language modeling\n",
    "- language modeling은 일반적으로 예제 (x1, x2, ..., xn)의 가변 길이 기호 (s1, s2, ..., sn) 시퀀스로 구성된 것들에서의 비지도 분포 추정으로 구성\n",
    "- 언어는 자연스럽게 순차적인 순서를 가지므로, 기호의 조건부 확률의 곱으로 합성물 확률을 인수 분해하는 것이 일반적(Jelinek & Mercer, 1980) (Bengio et al., 2003)\n",
    "- 이 접근법은 p(x) 및 p(sn-k, ..., sn|s1, ..., sn-k-1)과 같은 모든 조건부의 추정 및 샘플링을 가능하게 함\n",
    "- 최근 몇 년 동안, Transformer와 같은 self-attention 아키텍처를 비롯하여 이러한 조건부 확률을 계산할 수 있는 모델의 표현력이 크게 개선됨\n",
    "\n",
    "\n",
    "- single task를 수행하는 것을 조건부 분포 p(output|input)를 추정하는 확률적인 프레임워크로 나타낼 수 있음\n",
    "- 일반적인 시스템은 같은 입력에 대해 다양한 작업을 수행할 수 있어야 하므로, 입력뿐만 아니라 수행할 작업에 대해서도 조건을 설정해야 함\n",
    "- 즉, p(output|input, task)를 모델링해야 함\n",
    "- 이는 multi task 및 meta-learning task에서 각각 형식화 됨. 작업 조건은 종종 아키텍처 수준에서 구현됨 \n",
    "- (Kaiser et al., 2017)에서와 같은 작업별 인코더 및 디코더 또는 MAML (Finn et al., 2017)의 내부 및 외부 루프 최적화 프레임워크와 같은 알고리즘 수준에서 구현됨\n",
    "- 그러나 McCann et al. (2018)에서 나타난 것처럼 언어는 작업, 입력 및 출력을 모두 기호의 시퀀스로 지정하는 유연한 방법을 제공\n",
    "- 예를 들어, 번역 훈련 예제는 (프랑스어로 번역, 영어 텍스트, 프랑스어 텍스트) 시퀀스로 작성될 수 있음\n",
    "- 마찬가지로, 독해 학습 예제는 (질문에 대한 답변, 문서, 질문, 답변)와 같은 형식으로 작성할 수 있음\n",
    "- McCann et al. (2018)은 이러한 형식의 예제에서 많은 다른 작업을 추론하고 수행하는 단일 모델인 MQAN을 학습하는 것이 가능하다는 것을 입증함\n",
    "\n",
    "\n",
    "- language modeling은 원칙적으로 McCann 등이 제시한 작업을 명시적인 지도 없이도 학습할 수 있음\n",
    "- 지도 학습 목적은 비지도 학습 목적과 동일하지만 일부 시퀀스에서만 평가됨\n",
    "- 이러한 약간의 장난감 같은 설정에서는, (Sutskever et al., 2015)에서 논의된 원리적인 훈련 목적으로서의 밀도 추정에 대한 우려는 회피됨\n",
    "- 이 문제는 실제로 비지도 학습 목적을 수렴시킬 수 있는지 여부에 달려 있음\n",
    "- 예비 실험은 충분히 큰 언어 모델이 이러한 장난감적인 설정에서도 멀티태스킹 학습을 수행할 수 있음을 확인했지만, 학습 속도가 명시적인 지도 학습 방법보다 훨씬 느림\n",
    "\n",
    "\n",
    "- 위에서 설명한 잘 정의된 설정에서부터 \"language in the wild\"의 혼돈스러운 상황까지는 큰 단계\n",
    "- Weston (2016)은 대화의 맥락에서 직접 자연어를 학습할 수 있는 시스템의 필요성을 주장하며, teacher's output을 예측하여 보상 신호 없이 QA 작업을 학습하는 개념 증명을 보여줌\n",
    "- 대화는 매력적인 방법이지만, 지나치게 제한적이라는 우려가 있음\n",
    "- 인터넷에는 인터랙티브 커뮤니케이션 없이도 수집 가능한 방대한 양의 정보가 존재\n",
    "- 논문에서는 충분한 용량을 가진 언어 모델이, 조달 방법에 관계없이 자연어 시퀀스에서 시연된 작업을 더 잘 예측하기 위해 추론하고 수행하는 방법을 배우기 시작할 것이라는 것이라고 추정\n",
    "- 언어 모델이 이렇게 할 수 있다면, 실질적으로 비지도 멀티태스크 학습을 수행하게 됨\n",
    "- 논문에서는 다양한 작업에 대해 zeroshot 설정에서 언어 모델의 성능을 분석하여 이것이 사실인지 여부를 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5e9d18",
   "metadata": {},
   "source": [
    "## 2.1. Training Dataset\n",
    "\n",
    "- 과거의 대부분의 연구는 뉴스 기사(Jozefowicz et al., 2016), 위키피디아(Merity et al., 2016), 혹은 소설(Kiros et al., 2015)과 같이 하나의 텍스트 도메인에서 언어 모델을 훈련시키는 것이었음\n",
    "- 본 논문에서는 가능한 한 다양한 도메인과 맥락에서 자연어 작업을 수행하기 위해 가능한 한 크고 다양한 데이터셋을 구축\n",
    "\n",
    "\n",
    "- 다양하고 거의 무한한 텍스트의 유망한 소스는 Common Crawl과 같은 웹 스크랩\n",
    "- 다만, 데이터 품질 문제가 있음 \n",
    "- Trinh & Le(2018)은 공감 추론에 대한 작업에서 Common Crawl을 사용했지만, \"대부분 이해할 수 없는 내용의 문서\"가 많다고 지적\n",
    "- 논문에서도 Common Crawl을 사용한 초기 실험에서 유사한 데이터 문제를 관찰했다고 지적\n",
    "- Trinh & Le(2018)의 최고 결과는 대상 데이터셋인 Winograd Schema Challenge와 가장 유사한 문서만을 포함하는 작은 하위 샘플을 사용하여 달성되었음\n",
    "- 이것은 특정 작업에서 성능을 향상시키기 위한 실용적인 접근 방식이지만, 논문에서는 미리 수행할 작업에 대한 가정을 피하기 위해 사용하지 않음\n",
    "\n",
    "\n",
    "- 본 논문은 문서의 품질에 중점을 둔 새로운 웹 크롤링 데이터셋을 만듦\n",
    "- Reddit에서 적어도 3개의 카르마를 받은 아웃바운드 링크만 크롤링\n",
    "- 이는 다른 사용자들이 해당 링크를 흥미롭거나 교육적이거나 그냥 재미있게 생각하는지에 대한 heuristic indicator로 생각할 수 있음\n",
    "\n",
    "\n",
    "- 결과적으로 생성된 데이터셋인 WebText는 이 4,500만개 링크들의 텍스트 부분을 포함\n",
    "- HTML 응답에서 텍스트를 추출하기 위해 Dragnet (Peters & Lecocq, 2013)와 Newspaper1 컨텐츠 추출기를 조합하여 사용\n",
    "- 본 논문에서 제시된 모든 결과는 2017년 12월 이후에 생성된 링크를 포함하지 않는 WebText의 초기 버전을 사용하며, 중복을 제거하고 일부 휴리스틱 기반 클리닝을 거침\n",
    "- 이 버전은 약 8백만 개의 문서와 총 40GB의 텍스트를 포함\n",
    "- WebText에서 모든 위키피디아 문서를 제거\n",
    "    - 위키피디아는 다른 데이터셋의 공통 데이터 소스이며, 테스트 평가 작업에서 중복된 학습 데이터로 인해 분석이 복잡해질 수 있기 때문\n",
    "    \n",
    "<img src = \"./image/gpt2_webtext_trainingset.png\">\n",
    "- WebText training set에서 만들어진 영어-프랑스어 및 프랑스어-영어 번역의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a1376",
   "metadata": {},
   "source": [
    "## 2.2. Input Representation\n",
    "\n",
    "- 일반적인 언어 모델(LM)은 어떤 문자열의 확률을 계산할 수 있어야 함\n",
    "- 또한 해당 문자열을 생성할 수도 있어야 함\n",
    "- 현재 대규모 LMs는 대/소문자 구분, 토큰화 및 어휘가 없는 토큰과 같은 전처리 단계를 포함하고 있어, 모델 가능한 문자열의 공간을 제한하고 있음\n",
    "- 유니코드 문자열을 UTF-8 바이트 시퀀스로 처리하는 것은 Gillick et al. (2015) 등의 연구에서 보여주었듯이 이러한 요구 사항을 해결함\n",
    "- 그러나 현재 바이트 수준의 LMs는 One Billion Word Benchmark (Al-Rfou et al., 2018)와 같은 대규모 데이터셋에서 단어 수준의 LMs와 경쟁력이 없음\n",
    "- 본 논문에서는 WebText에서 표준 바이트 수준 LMs를 훈련시키는 시도에서도 유사한 성능 차이를 관찰함\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "- BPE는 문자와 단어 수준의 언어 모델링 사이의 실용적인 중간 지점\n",
    "- BPE는 빈번한 심볼 시퀀스에 대해서는 단어 수준 입력을, 드물게 나타나는 심볼 시퀀스에 대해서는 문자 수준 입력을 효과적으로 보관 \n",
    "- 그러나 BPE 구현에서 대개는 바이트 시퀀스 대신 유니코드 코드 포인트를 다룸\n",
    "- 이러한 구현은 모든 유니코드 문자열을 모델링하려면 모든 유니코드 심볼을 포함해야 함\n",
    "- 이는 다중 심볼 토큰이 추가되기 전에 기본 어휘 크기가 130,000개 이상이 됨을 의미\n",
    "- 이는 BPE에서 자주 사용되는 32,000~64,000개의 토큰 어휘와 비교하여 너무 큰 어휘 크기\n",
    "- 반면에, 바이트 수준의 BPE 버전은 기본 어휘 크기가 256개로 충분\n",
    "- 그러나 바이트 시퀀스에 직접 BPE를 적용하면 한정된 어휘 슬롯과 모델 용량으로 인해 부적절한 병합이 발생\n",
    "- 이는 \"dog\"와 같은 일반적인 단어의 여러 버전이 많이 포함되기 때문\n",
    "- 이러한 경우 제한된 어휘 슬롯 및 모델 용량 할당에 대한 부적절한 결과가 나타남\n",
    "\n",
    "\n",
    "- 이를 방지하기 위해 논문에서는 바이트 시퀀스에 대해 문자 범주를 넘나들며 BPE가 병합하지 못하게 함\n",
    "- 공백에 대해서는 예외를 추가하여 약간의 단어 분할을 추가하지만 압축 효율성을 크게 향상시킴\n",
    "- 이 입력 표현은 단어 수준 언어 모델의 경험적 이점과 바이트 수준 접근 방식의 일반성을 결합할 수 있게 해줌\n",
    "- 논문의 접근 방식은 모든 유니코드 문자열에 확률을 할당할 수 있으므로, 전처리, 토큰화 또는 어휘 크기와 관계없이 모든 데이터셋에서 언어 모델을 평가할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be1eb2",
   "metadata": {},
   "source": [
    "## 2.3. Model\n",
    "\n",
    "- 본 논문은 Transformer 기반의 아키텍처를 LM에 사용\n",
    "- 모델은 대부분 OpenAI GPT 모델 (Radford et al., 2018)의 구체적인 사항을 따름\n",
    "- 레이어 정규화 (Ba et al., 2016)는 각 서브 블록의 입력으로 이동되었으며, He et al. (2016)의 사전 활성화 잔여 네트워크와 유사하게 구성\n",
    "- 또한, 마지막 self-attention 블록 이후에 추가적인 레이어 정규화 실행\n",
    "- 모델 깊이에서 residual path 누적을 고려한 modified initialization가 사용됨. 초기화 시 잔여 레이어의 가중치를 1/√N (N은 잔여 레이어 수)의 배율로 조정\n",
    "- 어휘는 50,257로 확장되었으며, 컨텍스트 크기는 512에서 1024 토큰으로 증가되었으며, 더 큰 배치 크기 512가 사용됨\n",
    "\n",
    "<img src = \"./image/gpt2_params.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f7773",
   "metadata": {},
   "source": [
    "# 3. Experiments\n",
    "\n",
    "<img src = \"./image/gpt2_size.png\" width = 600>\n",
    "\n",
    "- 본 논문은 대략 log-uniformly spaced sizes의 4개의 LM을 학습하고 벤치마크를 진행\n",
    "- 가장 작은 모델은 원래 GPT와 동등하며, 두 번째로 작은 모델은 BERT의 가장 큰 모델과 동등함\n",
    "- 가장 큰 모델인 GPT-2라고 부르는 모델은 GPT보다 파라미터가 한 차례 더 많음\n",
    "- 각 모델의 학습률은 WebText의 5% 홀드아웃 샘플에서 최적의 perplexity를 수동으로 조정\n",
    "- 모든 모델은 WebText를 underfit하며, 더 많은 학습 시간이 주어져도 아직 홀드아웃 perplexity가 개선되지 않았음\n",
    "\n",
    "## 3.1. Language Modeling\n",
    "\n",
    "- WebText LM 모델이 훈련된 기본 작업, 즉 언어 모델링에서 제로샷 도메인 transfer에서 어떻게 수행되는지 이해하기 위해 초기 단계로 진행됨\n",
    "- 모델은 바이트 수준에서 작동하며 손실이 없는 전처리나 토큰화가 필요하지 않기 때문에 모든 언어 모델 벤치마크에서 평가할 수 있음\n",
    "- 일반적으로 언어 모델링 데이터셋의 결과는 정규화된 예측 단위(일반적으로 문자, 바이트 또는 단어)당 평균 음의 로그 확률의 스케일 또는 지수 버전으로 보고됨\n",
    "- 본 논문은 데이터셋의 로그 확률을 웹텍스트 언어 모델에 따라 계산하고 정규화된 단위 수로 나누어 동일한 수량을 계산\n",
    "- 이러한 데이터셋 중 많은 경우 WebText LM은 적극적으로 표준화된 텍스트, 연결되지 않은 구두점 및 축약어와 같은 토큰화 아티팩트, 섞인 문장, 심지어 WebText에서 극히 드문 [UNK] 문자열을 예측해야 하는 상황에서 범용성이 많이 떨어지게 됨 - occurring only 26 times in 40 billion bytes\n",
    "- 이러한 토큰화 / 전처리 아티팩트를 최대한 제거하는 디토크나이저를 사용하여 표에서 주요 결과를 보고\n",
    "- 이러한 디토크나이저는 역변환이 가능하기 때문에 데이터셋의 로그 확률을 계산할 수 있으며, 단순한 도메인 적응의 형태로 생각할 수 있음\n",
    "- 이러한 디토크나이저로 GPT-2의 2.5~5 perplexity 향상을 관찰함\n",
    "\n",
    "\n",
    "- WebText 언어 모델은 도메인과 데이터셋을 가로질러 잘 transfer되며, 제로샷(zero-shot) 설정에서 8개 데이터셋 중 7개에서 SOTA 달성\n",
    "- Penn Treebank 및 WikiText-2와 같은 작은 데이터셋에서는 1~2백만 개의 훈련 토큰만 있으며, 장기 의존성을 측정하는 데이터셋인 LAMBADA (Paperno et al., 2016)와 어린이 도서 테스트 (Children's Book Test) (Hill et al., 2015)에서 큰 개선이 이루어짐\n",
    "- 그러나 One Billion Word Benchmark (Chelba et al., 2013)에서는 이전의 연구보다 여전히 현저하게 성능이 낮음 \n",
    "- 이것은 아마도 가장 큰 데이터셋이면서 가장 파괴적인 전처리 중 일부를 갖고 있는 1BW 때문일 것으로 추정\n",
    "- 1BW의 문장 수준 셔플링은 모든 장거리 구조를 제거\n",
    "\n",
    "<img src = \"./image/gpt2_zeroshot2.png\">\n",
    "-  Zero-shot results on many datasets. No training or fine-tuning was performed for any of these results. PTB and WikiText-2 results are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018) and LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e072fe5",
   "metadata": {},
   "source": [
    "## 3.2. Children’s Book Test\n",
    "\n",
    "<img src = \"./image/gpt2_cbt.png\" width = 500>\n",
    "- Performance on the Children’s Book Test as a function of model capacity. Human performance are from Bajgar et al. (2016), instead of the much lower estimates from the original paper.\n",
    "\n",
    "\n",
    "- 어린이 도서 테스트(CBT)는 LMs가 명사, 동사, 전치사, 고유 명사와 같은 다양한 단어 카테고리에서 얼마나 잘 수행되는지 살펴보기 위해 만들어짐\n",
    "- 평가 지표로 퍼플렉서티(perplexity) 대신, CBT는 생략된 단어의 10가지 가능한 선택지 중 올바른 것을 예측하는 자동 생성된 클로즈 테스트의 정확도를 보고\n",
    "- 원래 논문에서 소개된 LM 접근 방식을 따라, 각 선택지와 나머지 문장을 LM에 따라 조건부 확률을 계산하고, 가장 높은 확률을 가진 것을 예측\n",
    "- 그림에서 볼 수 있듯이, 모델 크기가 증가함에 따라 성능이 점차 개선되며 이 테스트에서 인간 수준의 성능에 근접\n",
    "- 데이터 중복 분석에 따르면, CBT 테스트 세트 중 하나인 Rudyard Kipling의 The Jungle Book은 WebText에 포함되어 있으므로 중요한 중복이 있는 데이터가 아닌 검증 세트에서 결과를 보고\n",
    "- GPT-2는 고유 명사에서 89.1%, 일반 명사에서 93.3%의 최고 성과를 기록하여 새로운 최고 성과를 달성\n",
    "- CBT에서는 PTB 스타일의 토큰화 구조가 제거되도록 디토크나이저가 적용됨\n",
    "\n",
    "\n",
    "## 3.3. LAMBADA\n",
    "\n",
    "- LAMBADA 데이터셋(Paperno et al., 2016)은 시스템이 텍스트의 장거리 의존성을 모델링할 수 있는 능력을 시험\n",
    "- 이 과제는 인간이 성공적으로 예측하기 위해 적어도 50개의 토큰 컨텍스트가 필요한 문장의 마지막 단어를 예측하는 것\n",
    "- GPT-2는 이전 연구(Grave et al., 2016)의 99.8 perplexity를 8.6 perplexity로 개선하였으며, LM의 정확도도 19% (Dehghani et al., 2018)에서 52.66%로 증가시킴\n",
    "- GPT-2의 오류를 조사한 결과, 대부분의 예측은 문장의 유효한 연속성을 유지하지만, 유효한 마지막 단어가 아님\n",
    "- 이는 LM이 단어가 문장의 마지막 단어여야 한다는 추가적인 유용한 제약 조건을 사용하지 않기 때문\n",
    "- 이를 근사하기 위해 stop-word 필터를 추가하면 정확도가 63.24%로 더 개선됨\n",
    "- 이를 통해 이 과제의 전반적인 성과를 4% 개선하여 이전의 최고 성과(Hoang et al., 2018)를 능가함\n",
    "- 이전 연구에서는 모델의 출력을 컨텍스트에 나타난 단어로 제한하는 다른 제한된 예측 설정을 사용\n",
    "- 그러나 GPT-2의 경우, 19%의 정답이 컨텍스트 내에 없기 때문에 이러한 제한은 오히려 해로움. 따라서 전처리되지 않은 버전의 데이터셋을 사용\n",
    "\n",
    "\n",
    "## 3.4. Winograd Schema Challenge\n",
    "\n",
    "<img src = \"./image/gpt2_wsc.png\">\n",
    "\n",
    "- Winograd Schema Challenge(Levesque et al., 2012)는 시스템이 텍스트의 모호성을 해결하는 능력을 측정하여 상식적 추론 능력을 측정하기 위해 구축됨\n",
    "- 최근 Trinh & Le(2018)은 LMs를 사용하여 모호성을 해결하는 것을 더 높은 확률로 예측하여 이 도전 과제에 대한 중요한 진전을 이룸\n",
    "- 본 논문은 그들의 문제 정의를 따르고 전체 및 부분 점수 기술로 모델의 성능을 시각화하여 그림에 나타냄\n",
    "- GPT-2는 70.70%의 정확도를 달성하여 상태에서 가장 높은 결과를 달성\n",
    "- 이 데이터셋은 273개의 예제로 매우 작기 때문에 이 결과를 이해하기 위해 Trichelair et al.(2018)를 읽는 것을 권장\n",
    "\n",
    "\n",
    "## 3.5. Reading Comprehension\n",
    "\n",
    "- The Conversation Question Answering dataset(CoQA) 데이터셋(Reddy et al., 2018)은 7가지 다른 도메인의 문서와 해당 문서에 대한 질문자와 응답자 간의 대화를 포함하는 자연어 대화 쌍으로 이루어져 있음\n",
    "- CoQA는 독해 능력을 테스트하며 \"왜?\"와 같이 대화 내력에 따라 답변을 제공해야 하는 질문에 대한 모델의 능력도 테스트\n",
    "- GPT-2를 문서, 연관 대화 내력 및 최종 토큰 A에 조건부로 Greedy Decoding을 수행한 결과, 개발 세트에서 55 F1의 성능을 달성\n",
    "- 이는 3개의 베이스라인 모델 중 3개의 성능을 상회하거나 일치시키며, 이러한 베이스라인은 수동으로 수집한 127,000개 이상의 질문-답변 쌍을 학습한 모델들\n",
    "- 지도 학습을 기반으로 한 SOTA인 BERT 기반 시스템(Devlin et al., 2018)은 인간의 89 F1 성능에 근접\n",
    "- GPT-2의 성능은 지도 학습이 없는 시스템으로서는 흥미로운 결과이지만, 답변과 오류를 조사해보면, GPT-2는 누구에 대한 질문에 대해서는 문서에서 이름을 찾아 답변하는 등 간단한 검색 기반의 휴리스틱을 사용하는 경향이 있음\n",
    "\n",
    "\n",
    "## 3.6. Summarization\n",
    "\n",
    "<img src = \"./image/gpt2_summarization.png\">\n",
    "\n",
    "- GPT-2의 요약 능력을 CNN과 Daily Mail 데이터셋 (Nallapati et al., 2016)에서 테스트\n",
    "- 기사 뒤에 텍스트 TL;DR:을 추가하고, k = 2로 설정한 Top-k 랜덤 샘플링(Fan et al., 2018)을 이용해 100개의 토큰을 생성하여, 반복을 줄이고 Greedy Decoding보다 더 추상적인 요약을 유도\n",
    "- 이 100개의 토큰 중에서 첫 3개의 문장을 요약으로 사용\n",
    "- 표에서 보여지는 것과 같이, 결과물은 요약을 닮아 있으나, 종종 기사의 최근 내용에 초점을 맞추거나, 충돌 사고에 참여한 자동차의 대수나 모자 또는 셔츠에 로고가 있는지와 같은 구체적인 세부 사항을 혼동\n",
    "- 일반적으로 보고된 ROUGE 1,2,L 메트릭에서 생성된 요약은 고전적인 신경망 기반 모델과 비교해서 성능이 낮으며, 기사에서 임의로 3개의 문장을 선택하는 것만큼 겨우 능가 \n",
    "- task hint를 제거하면 GPT-2의 성능은 평균 metric에서 6.4 포인트 하락하여, 자연어를 사용한 언어 모델의 과제별 특정 동작을 활성화하는 능력을 보여줌\n",
    "\n",
    "\n",
    "## 3.7. Translation\n",
    "\n",
    "- GPT-2가 다른 언어로 번역하는 것을 배우기 시작했는지 테스트\n",
    "- 이를 원하는 작업으로 유도하기 위해, 논문은 영어 문장 = 프랑스어 문장 형식의 예시 세트 문맥에서 언어 모델을 조건부로 설정하고, 마지막 프롬프트 후에 영어 문장 =로 샘플링 한 후, 생성된 첫 번째 문장을 번역으로 사용\n",
    "- WMT-14 영어-프랑스어 테스트 세트에서 GPT-2는 5 BLEU를 얻음 \n",
    "- 이는 비지도 단어 번역에 대한 이전 연구에서 추론 된 양방향 어휘 목록을 사용한 단어 대 단어 대체보다 약간 더 나쁨 (Conneau 등, 2017b)\n",
    "- WMT-14 프랑스어-영어 테스트 세트에서 GPT-2는 매우 강력한 영어 언어 모델을 활용하여 훨씬 더 나은 결과를 얻어 11.5 BLEU를 달성\n",
    "- 이는 (Artetxe 등, 2017) 및 (Lample 등, 2017)의 여러 비지도 기계 번역 기준을 능가하지만 현재 최고의 비지도 기계 번역 접근 방식인 (Artetxe 등, 2019)의 33.5 BLEU에는 크게 뒤쳐짐\n",
    "- 이 작업에서의 성능은 WebText에서 비-영어 웹 페이지를 의도적으로 필터링 단계로 제거했기 때문에 놀라운 결과\n",
    "- 이를 확인하기 위해, WebText에서 byte-level language detector2를 실행하여 프랑스어로만 된 데이터가 10MB밖에 없다는 것을 감지했으며, 이는 이전 비지도 기계 번역 연구에서 사용되는  monolingual French corpus보다 약 500배 작음\n",
    "\n",
    "\n",
    "## 3.8.  Question Answering\n",
    "\n",
    "<img src = \"./image/gpt2_qa.png\">\n",
    "\n",
    "- 언어 모델 내에 어떤 정보가 포함되어 있는지 확인하는 가능한 방법 중 하나는 사실 기반 질문에 대한 정확한 답변을 생성하는 빈도를 평가하는 것\n",
    "- 모든 정보가 매개변수에 저장되는 신경 시스템에서 이러한 동작을 이전에 증명한 A Neural Conversational Model(Vinyals & Le, 2015)과 같은 경우, 고품질 평가 데이터셋의 부족으로 인해 질적 결과가 보고됨 \n",
    "- 최근 소개된 자연 질문(Natural Questions) 데이터셋(Kwiatkowski et al., 2019)은 이러한 작업을 보다 양적으로 테스트하는 데 유망한 자원\n",
    "- 번역과 마찬가지로, 언어 모델의 문맥은 예시 질문-답변 세트로 시작되어 데이터셋의 짧은 답변 스타일을 모델이 추론하는 데 도움이 됨\n",
    "- GPT-2는 SQUAD와 같은 독해 데이터셋에서 일반적으로 사용되는 정확 일치 측정 항목으로 평가될 때 4.1%의 질문에 대해 정확한 답변을 제공\n",
    "- 비교적으로, 가장 작은 모델은 각 질문 유형(who, what, where 등)의 가장 일반적인 답변을 반환하는 매우 간단한 기준의 1.0% 정확도를 초과하지 못함\n",
    "- GPT-2는 질문에 대해 5.3배 더 많은 정확한 답변을 제공하며, 모델 용량이 이러한 유형의 작업에서 신경 시스템의 성능 저하에 주요한 요소였음을 시사\n",
    "- GPT-2가 생성한 답변에 할당하는 확률은 잘 보정되어 있으며, GPT-2는 가장 자신감이 높은 1%의 질문에서 63.1%의 정확도를 가짐\n",
    "- 개발 세트 질문에서 GPT-2가 생성한 가장 자신감이 높은 상위 30개의 답변은 표에 나와 있음\n",
    "- GPT-2의 성능은 여전히 추출적 문서 질문 답변과 정보 검색을 하이브리드화한 일반적인 영역 질문 응답 시스템의 30~50% 범위보다 훨씬 낮음(Alberti et al., 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97e90f",
   "metadata": {},
   "source": [
    "# 4. Generalization vs Memorization\n",
    "\n",
    "- 최근 컴퓨터 비전 분야에서의 연구는 일반적인 이미지 데이터셋이 유사한 이미지를 상당 부분 포함하고 있음을 보여줌\n",
    "- 예를 들어, CIFAR-10 데이터셋은 학습 이미지와 테스트 이미지 사이에 3.3%의 중복이 있다는 것을 보여준 연구가 있음 (Barz & Denzler, 2019)\n",
    "- 이는 기계 학습 시스템의 일반화 성능을 과대평가하게 됨 \n",
    "- 데이터셋의 크기가 증가함에 따라 이러한 문제는 더욱 발생할 가능성이 있으므로 WebText에서도 유사한 현상이 발생할 가능성이 있음\n",
    "- 따라서 테스트 데이터가 학습 데이터에 얼마나 포함되어 있는지 분석하는 것이 중요함\n",
    "\n",
    "\n",
    "- 이를 연구하기 위해, 본 논문은 WebText 훈련 세트 토큰의 8-grams를 포함하는 Bloom 필터를 만듦\n",
    "- 검색률을 개선하기 위해 문자열은 소문자 알파벳 단어만 단일 공백 구분자로 포함되도록 정규화\n",
    "- Bloom 필터는 잘못된 양성율이 1/10^8보다 크지 않도록 구성됨\n",
    "- 논문은 또한 필터에서 찾을 수 있는 문자열이 없도록 1백만 개의 문자열을 생성하여 잘못된 양성율이 매우 낮다는 것을 확인\n",
    "\n",
    "\n",
    "- 이러한 Bloom 필터는 데이터셋이 주어졌을 때, 해당 데이터셋의 8-gram 중 WebText 훈련 세트에서 찾을 수 있는 8-gram의 백분율을 계산할 수 있게 해줌\n",
    "- 표는 일반 언어 모델 벤치마크의 테스트 세트에 대한 이러한 중복 분석을 보여줌\n",
    "- 일반적인 언어 모델 데이터셋의 테스트 세트는 WebText 훈련 세트와 1-6%의 중복이 있으며, 평균적으로는 3.2%의 중복이 있음\n",
    "- 다소 놀라운 것은 많은 데이터셋이 자체 훈련 세트와 더 큰 중복이 있음\n",
    "- 이 경우 중복 분석은 recall을 최적화하도록 구성되어 있으며, 중복 검사의 수동 검토에서 많은 일반적인 구문이 있지만, 중복된 데이터 때문에 긴 일치도 많이 있음\n",
    "- 이는 WebText에만 해당되는 것이 아니라, WikiText-103의 테스트 세트에는 훈련 데이터셋에도 있는 기사가 포함되어 있음\n",
    "- 테스트 세트에는 60개의 기사만 있으므로 최소한 1.6%의 중복이 있음\n",
    "- 더 걱정스러운 것은 1BW가 절차에 따라 자체 훈련 세트와 거의 13.2%의 중복이 있다는 것\n",
    "\n",
    "\n",
    "- 이를 조사하기 위해, 본 논문은 WebText 학습 데이터 세트의 8-gram을 포함하는 Bloom 필터를 생성\n",
    "- 회수율을 개선하기 위해 문자열은 소문자 알파벳 단어와 단일 공백으로만 구성되도록 표준화됨\n",
    "- Bloom 필터는 거짓 양성 비율이 1/10^8 이하가 되도록 구성됨\n",
    "- 논문은 또한 1백만 개의 문자열을 생성하여 필터에서 찾을 수 있는 문자열이 없음을 확인함으로써 거짓 양성 비율을 검증\n",
    "- 결과는 표에서 확인 가능\n",
    "\n",
    "<img src = \"./image/gpt2_overlapping.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
